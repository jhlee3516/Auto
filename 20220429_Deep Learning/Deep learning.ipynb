{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "유튜브1: https://youtu.be/pIP6QvUshww  \n",
    "유튜브2: https://youtu.be/ePLR3clfH6g  \n",
    "유튜브3: https://youtu.be/blb3pmBJLRc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ① MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21457ca59e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0427 19:00:13.471171  8584 deprecation.py:506] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.2653 - acc: 0.9230\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.1088 - acc: 0.9668\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0741 - acc: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21457c50a58>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 29us/sample - loss: 0.0958 - acc: 0.9715\n",
      "0.09578466763249599\n",
      "0.9715\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(X_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0427 19:06:44.286809  8584 deprecation.py:506] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0427 19:06:44.288812  8584 deprecation.py:506] From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0427 19:06:44.461014  8584 hdf5_format.py:263] Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('mnist.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_model.predict([X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ② 주식단타매매\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>주문일자</th>\n",
       "      <th>종목명</th>\n",
       "      <th>매수가격</th>\n",
       "      <th>매수시간</th>\n",
       "      <th>매도시간</th>\n",
       "      <th>수익률</th>\n",
       "      <th>매매횟수</th>\n",
       "      <th>종목코드</th>\n",
       "      <th>뉴스기사</th>\n",
       "      <th>최대거래대금 시간</th>\n",
       "      <th>최대거래대금</th>\n",
       "      <th>매수등락률</th>\n",
       "      <th>시가등락률</th>\n",
       "      <th>양봉개수</th>\n",
       "      <th>직전 거래대금</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>고려시멘트</td>\n",
       "      <td>5230</td>\n",
       "      <td>2022-04-26 09:09:07</td>\n",
       "      <td>09:21:27</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1</td>\n",
       "      <td>198440.0</td>\n",
       "      <td>1</td>\n",
       "      <td>09:06:00</td>\n",
       "      <td>113</td>\n",
       "      <td>16.22</td>\n",
       "      <td>6.22</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>고려시멘트</td>\n",
       "      <td>5410</td>\n",
       "      <td>2022-04-26 09:28:09</td>\n",
       "      <td>09:32:19</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2</td>\n",
       "      <td>198440.0</td>\n",
       "      <td>1</td>\n",
       "      <td>09:06:00</td>\n",
       "      <td>113</td>\n",
       "      <td>20.22</td>\n",
       "      <td>6.22</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>영진약품</td>\n",
       "      <td>5490</td>\n",
       "      <td>2022-04-26 09:51:11</td>\n",
       "      <td>09:55:24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>3</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>1</td>\n",
       "      <td>09:50:00</td>\n",
       "      <td>62</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>버킷스튜디오</td>\n",
       "      <td>5060</td>\n",
       "      <td>2022-04-26 09:58:16</td>\n",
       "      <td>09:59:56</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>4</td>\n",
       "      <td>66410.0</td>\n",
       "      <td>1</td>\n",
       "      <td>09:22:00</td>\n",
       "      <td>72</td>\n",
       "      <td>11.58</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>우리바이오</td>\n",
       "      <td>4390</td>\n",
       "      <td>2022-04-26 10:00:40</td>\n",
       "      <td>10:00:50</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>82850.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10:00:00</td>\n",
       "      <td>44</td>\n",
       "      <td>10.86</td>\n",
       "      <td>4.17</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         주문일자     종목명  매수가격                 매수시간      매도시간   수익률  매매횟수  \\\n",
       "0  2022-03-29   고려시멘트  5230  2022-04-26 09:09:07  09:21:27  2.57     1   \n",
       "1  2022-03-29   고려시멘트  5410  2022-04-26 09:28:09  09:32:19  1.55     2   \n",
       "2  2022-03-29    영진약품  5490  2022-04-26 09:51:11  09:55:24  0.36     3   \n",
       "3  2022-03-29  버킷스튜디오  5060  2022-04-26 09:58:16  09:59:56 -0.89     4   \n",
       "4  2022-03-29   우리바이오  4390  2022-04-26 10:00:40  10:00:50 -0.87     5   \n",
       "\n",
       "       종목코드  뉴스기사 최대거래대금 시간  최대거래대금  매수등락률  시가등락률  양봉개수  직전 거래대금  \n",
       "0  198440.0     1  09:06:00     113  16.22   6.22     1       63  \n",
       "1  198440.0     1  09:06:00     113  20.22   6.22     2       16  \n",
       "2    3520.0     1  09:50:00      62   6.60   0.19     1       62  \n",
       "3   66410.0     1  09:22:00      72  11.58   3.09     0       26  \n",
       "4   82850.0     1  10:00:00      44  10.86   4.17     3       43  "
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['수익률'] = df['수익률'].apply(lambda x : 1 if x > 0 else 0 )\n",
    "df['수익률'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['뉴스기사', '직전 거래대금', '최대거래대금', '양봉개수']]\n",
    "y = df['수익률']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 4), (134,))"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer = Adagrad(lr=0.2)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_160 (Dense)            (None, 12)                60        \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.6873 - acc: 0.5000\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6504 - acc: 0.6300\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.6061 - acc: 0.7000\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.5715 - acc: 0.7600\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.5048 - acc: 0.7600\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.5389 - acc: 0.6900\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 0s 110us/step - loss: 0.5058 - acc: 0.7500\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.5012 - acc: 0.7200\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.4707 - acc: 0.7400\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.4795 - acc: 0.7300\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4517 - acc: 0.7500\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.4423 - acc: 0.7700\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4344 - acc: 0.7500\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.4305 - acc: 0.7700\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4404 - acc: 0.7400\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.4378 - acc: 0.7500\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.4345 - acc: 0.7500\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4300 - acc: 0.7400\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4209 - acc: 0.7500\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.4050 - acc: 0.7700\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.4295 - acc: 0.7400\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.4185 - acc: 0.7600\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4081 - acc: 0.7600\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4000 - acc: 0.7600\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4055 - acc: 0.7600\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3903 - acc: 0.7700\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.4016 - acc: 0.7700\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.3914 - acc: 0.7600\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3947 - acc: 0.7800\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3852 - acc: 0.7800\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3981 - acc: 0.7600\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3912 - acc: 0.7600\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.3987 - acc: 0.7500\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.4038 - acc: 0.7500\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3920 - acc: 0.7800\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3811 - acc: 0.7600\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3945 - acc: 0.7800\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3855 - acc: 0.7600\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3690 - acc: 0.7800\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3737 - acc: 0.7600\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3785 - acc: 0.7100\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.3786 - acc: 0.7700\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3734 - acc: 0.7700\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.3678 - acc: 0.7600\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.3757 - acc: 0.7500\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3669 - acc: 0.7500\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.3696 - acc: 0.7700\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3634 - acc: 0.7700\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3610 - acc: 0.7900\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3680 - acc: 0.7800\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3615 - acc: 0.7400\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.3736 - acc: 0.7400\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3651 - acc: 0.7600\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.3620 - acc: 0.7800\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3606 - acc: 0.7700\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.3576 - acc: 0.7600\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3482 - acc: 0.7500\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3679 - acc: 0.7000\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3542 - acc: 0.7700\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3458 - acc: 0.7600\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3508 - acc: 0.7400\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3409 - acc: 0.7500\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3425 - acc: 0.7800\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3434 - acc: 0.7600\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3461 - acc: 0.7800\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3451 - acc: 0.7800\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3409 - acc: 0.7800\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3370 - acc: 0.7800\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3337 - acc: 0.7800\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3311 - acc: 0.7900\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3321 - acc: 0.7800\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.3319 - acc: 0.7600\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3312 - acc: 0.7800\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3378 - acc: 0.8100\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3403 - acc: 0.7400\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3244 - acc: 0.8000\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3195 - acc: 0.8300\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3211 - acc: 0.7600\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3532 - acc: 0.7800\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3191 - acc: 0.8100\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3218 - acc: 0.8200\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.3204 - acc: 0.7900\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3216 - acc: 0.8000\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3154 - acc: 0.8100\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3119 - acc: 0.8300\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3100 - acc: 0.8000\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3103 - acc: 0.8000\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3135 - acc: 0.8000\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3059 - acc: 0.8200\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3097 - acc: 0.8000\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3044 - acc: 0.8200\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3045 - acc: 0.8100\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3133 - acc: 0.8200\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3071 - acc: 0.8000\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3079 - acc: 0.8100\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.3021 - acc: 0.8100\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3055 - acc: 0.8200\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3074 - acc: 0.8000\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2964 - acc: 0.8300\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3032 - acc: 0.8000\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3095 - acc: 0.7900\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3026 - acc: 0.8200\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2976 - acc: 0.8200\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2978 - acc: 0.8300\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3004 - acc: 0.8200\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3024 - acc: 0.8200\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2972 - acc: 0.8200\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3028 - acc: 0.8200\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2907 - acc: 0.8300\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3008 - acc: 0.8000\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.3014 - acc: 0.8200\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.3017 - acc: 0.8100\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2899 - acc: 0.8300\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2909 - acc: 0.8200\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2984 - acc: 0.8300\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2865 - acc: 0.8200\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2946 - acc: 0.8100\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2895 - acc: 0.8300\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2882 - acc: 0.8200\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2853 - acc: 0.8200\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2833 - acc: 0.8300\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2845 - acc: 0.8200\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2867 - acc: 0.8400\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2955 - acc: 0.7900\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2829 - acc: 0.8200\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2911 - acc: 0.8200\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2850 - acc: 0.8300\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2846 - acc: 0.8300\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2804 - acc: 0.8300\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2879 - acc: 0.8300\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2865 - acc: 0.8300\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2768 - acc: 0.8200\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2949 - acc: 0.8200\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2790 - acc: 0.8300\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2863 - acc: 0.8200\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2754 - acc: 0.8300\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2737 - acc: 0.8200\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2790 - acc: 0.8200\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2767 - acc: 0.8100\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.2699 - acc: 0.8100\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.2712 - acc: 0.8300\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2678 - acc: 0.8400\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2635 - acc: 0.8300\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2589 - acc: 0.8400\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2929 - acc: 0.7800\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2639 - acc: 0.8200\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2657 - acc: 0.8200\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2591 - acc: 0.8200\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2583 - acc: 0.8300\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2615 - acc: 0.8200\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2639 - acc: 0.8300\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2581 - acc: 0.8300\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2552 - acc: 0.8400\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2642 - acc: 0.8100\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2573 - acc: 0.8300\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2570 - acc: 0.8300\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2572 - acc: 0.8100\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2586 - acc: 0.8100\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2846 - acc: 0.8100\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2653 - acc: 0.8100\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2597 - acc: 0.8300\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2542 - acc: 0.8300\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2514 - acc: 0.8300\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2519 - acc: 0.8300\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2519 - acc: 0.8200\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2549 - acc: 0.8300\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 70us/step - loss: 0.2561 - acc: 0.8300\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2500 - acc: 0.8300\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2500 - acc: 0.8200\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2539 - acc: 0.8400\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2593 - acc: 0.8400\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2576 - acc: 0.8000\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2495 - acc: 0.8400\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2486 - acc: 0.8200\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2502 - acc: 0.8400\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2473 - acc: 0.8400\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.2481 - acc: 0.8400\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2471 - acc: 0.8300\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2557 - acc: 0.8100\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.2458 - acc: 0.8400\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2440 - acc: 0.8200\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2452 - acc: 0.8300\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.2506 - acc: 0.8200\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2452 - acc: 0.8400\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.2437 - acc: 0.8500\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 0s 110us/step - loss: 0.2459 - acc: 0.8200\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2129 - acc: 0.875 - 0s 110us/step - loss: 0.2445 - acc: 0.8500\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.2461 - acc: 0.8200\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.2421 - acc: 0.8500\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2441 - acc: 0.8000\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.2583 - acc: 0.8100\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 0s 140us/step - loss: 0.2485 - acc: 0.8400\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 0s 110us/step - loss: 0.2450 - acc: 0.8100\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 0s 120us/step - loss: 0.2437 - acc: 0.8400\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 0s 110us/step - loss: 0.2426 - acc: 0.8100\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 0s 110us/step - loss: 0.2404 - acc: 0.7900\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 0s 130us/step - loss: 0.2441 - acc: 0.8300\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2229 - acc: 0.800 - 0s 100us/step - loss: 0.2402 - acc: 0.8400\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 0s 90us/step - loss: 0.2426 - acc: 0.8400\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 0s 100us/step - loss: 0.2413 - acc: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22685e28748>"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, batch_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 24ms/step\n",
      "Accuracy: 55.88\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.rint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.,  0.,  1.])"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([-1.7, 0.2, 1.4])\n",
    "np.rint(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  4],\n",
       "       [11,  6]], dtype=int64)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, np.rint(y_pred))\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[y_test == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6176470588235294"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12+9)/34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "뉴스기사, 직전 거래대금, 최대거래대금, 양봉개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999976]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[1, 10, 75.73, 3]])\n",
    "scaled_input = scaler.transform(input)\n",
    "predictions = model.predict(scaled_input)\n",
    "print(predictions)\n",
    "print(np.rint(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5512874e-06]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "input = np.array([[0, 25, 199.53, 0]])\n",
    "scaled_input = scaler.transform(input)\n",
    "predictions = model.predict(scaled_input)\n",
    "print(predictions)\n",
    "print(np.rint(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ③ Tuning Hyperparameter\n",
    "grid search: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Tune Batch Size and Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adagrad(lr=0.2)    \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.650000 using {'batch_size': 40, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [100, 200, 300, 400]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.650000 using {'batch_size': 40, 'epochs': 200}\n",
      "0.610000 (0.045510) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.620000 (0.031357) with: {'batch_size': 10, 'epochs': 200}\n",
      "0.620000 (0.096199) with: {'batch_size': 10, 'epochs': 300}\n",
      "0.560000 (0.107561) with: {'batch_size': 10, 'epochs': 400}\n",
      "0.610000 (0.045510) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.630000 (0.087030) with: {'batch_size': 20, 'epochs': 200}\n",
      "0.590000 (0.042763) with: {'batch_size': 20, 'epochs': 300}\n",
      "0.580000 (0.088154) with: {'batch_size': 20, 'epochs': 400}\n",
      "0.650000 (0.076259) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.650000 (0.086190) with: {'batch_size': 40, 'epochs': 200}\n",
      "0.600000 (0.056249) with: {'batch_size': 40, 'epochs': 300}\n",
      "0.530000 (0.074397) with: {'batch_size': 40, 'epochs': 400}\n",
      "0.570000 (0.059948) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.570000 (0.056661) with: {'batch_size': 60, 'epochs': 200}\n",
      "0.570000 (0.056661) with: {'batch_size': 60, 'epochs': 300}\n",
      "0.620000 (0.095221) with: {'batch_size': 60, 'epochs': 400}\n",
      "0.630000 (0.081037) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.610000 (0.047709) with: {'batch_size': 80, 'epochs': 200}\n",
      "0.640000 (0.074028) with: {'batch_size': 80, 'epochs': 300}\n",
      "0.560000 (0.061552) with: {'batch_size': 80, 'epochs': 400}\n",
      "0.610000 (0.087924) with: {'batch_size': 100, 'epochs': 100}\n",
      "0.590000 (0.055144) with: {'batch_size': 100, 'epochs': 200}\n",
      "0.600000 (0.102160) with: {'batch_size': 100, 'epochs': 300}\n",
      "0.600000 (0.081212) with: {'batch_size': 100, 'epochs': 400}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Tune the Training Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.660000 using {'optimizer': 'Adagrad'}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=400, batch_size=40, verbose=0)\n",
    "\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.600000 (0.081212) with: {'optimizer': 'SGD'}\n",
      "0.640000 (0.068266) with: {'optimizer': 'RMSprop'}\n",
      "0.660000 (0.074437) with: {'optimizer': 'Adagrad'}\n",
      "0.610000 (0.067048) with: {'optimizer': 'Adadelta'}\n",
      "0.610000 (0.074059) with: {'optimizer': 'Adam'}\n",
      "0.620000 (0.076366) with: {'optimizer': 'Adamax'}\n",
      "0.600000 (0.081212) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Tune Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adagrad(lr=learn_rate)\n",
    "#     optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.660000 using {'learn_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "# param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.460000 (0.055570) with: {'learn_rate': 0.001}\n",
      "0.600000 (0.123381) with: {'learn_rate': 0.01}\n",
      "0.610000 (0.087924) with: {'learn_rate': 0.1}\n",
      "0.660000 (0.086971) with: {'learn_rate': 0.2}\n",
      "0.640000 (0.066881) with: {'learn_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
